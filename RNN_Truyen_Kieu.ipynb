{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN_Truyen_Kieu.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kWulw0zVV3C",
        "colab_type": "code",
        "outputId": "37ff6c56-7584-4183-f0b6-822f0177d2c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import re\n",
        "import numpy as np\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9dvgqfKD2be",
        "colab_type": "text"
      },
      "source": [
        "# Read file truyen_Kieu.txt and clean text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQqTJOELipoT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = '/content/drive/My Drive/dataset/truyen_Kieu.txt'\n",
        "import re\n",
        "\n",
        "with open(path) as file_text:\n",
        "  i = 0\n",
        "  text = file_text.read()\n",
        "  text = re.sub('[0-9]\\.? ?\\.? ?|[\\.,?!]','', text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-5cVYOqD07d",
        "colab_type": "code",
        "outputId": "7b17ff41-cea5-472f-c090-9663113a1420",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print('Length of text : {} characters'.format(len(text)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text : 101360 characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EtSl1pDyFj1W",
        "colab_type": "code",
        "outputId": "1d2eef9f-3c99-4343-f6ea-4eb8b65f4da1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "vocab = sorted(set(text))\n",
        "print ('{} unique characters'.format(len(vocab)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "125 unique characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJgVS1hIzgcG",
        "colab_type": "text"
      },
      "source": [
        "# Create vocabulary (include space and newline character)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snmkYeplM1g-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating a mapping from unique characters to indices\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "text_as_int = np.array([char2idx[c] for c in text])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ep86jbQNGKa",
        "colab_type": "code",
        "outputId": "fdc9c34f-87fe-4182-e15b-4fa8674eb967",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "print('{')\n",
        "for char,_ in zip(char2idx, range(20)):\n",
        "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
        "print('  ...\\n}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "  '\\n':   0,\n",
            "  ' ' :   1,\n",
            "  \"'\" :   2,\n",
            "  '-' :   3,\n",
            "  ':' :   4,\n",
            "  ';' :   5,\n",
            "  'A' :   6,\n",
            "  'B' :   7,\n",
            "  'C' :   8,\n",
            "  'D' :   9,\n",
            "  'E' :  10,\n",
            "  'G' :  11,\n",
            "  'H' :  12,\n",
            "  'K' :  13,\n",
            "  'L' :  14,\n",
            "  'M' :  15,\n",
            "  'N' :  16,\n",
            "  'O' :  17,\n",
            "  'P' :  18,\n",
            "  'Q' :  19,\n",
            "  ...\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUPVpMjXNJkD",
        "colab_type": "code",
        "outputId": "35bd87b8-9d04-4bb1-f4a1-0c49cf805e47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Show how the first 14 characters from the text are mapped to integers\n",
        "print ('{} ---- characters mapped to int ---- > {}'.format(repr(text[:14]), text_as_int[:14]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'Trăm năm trong' ---- characters mapped to int ---- > [22 43 70 38  1 39 70 38  1 45 43 40 39 33]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ae4HbqjINw5-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The maximum length sentence we want for a single input in characters\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "\n",
        "# Create training examples / targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snvPdtFj4h6k",
        "colab_type": "code",
        "outputId": "829adde9-ddbf-4527-f277-786374fdf0db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "sequences = char_dataset.batch(seq_length, drop_remainder=True)\n",
        "\n",
        "for item in sequences.take(5):\n",
        "  print(repr(''.join(idx2char[item.numpy()])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'Trăm năm trong cõi người ta\\nChữ tài chữ mệnh khéo là ghét nhau\\nTrải qua một cuộc bể dâu\\nNhững điều t'\n",
            "'rông thấy mà đau đớn lòng\\nLạ gì bỉ sắc tư phong\\nTrời xanh quen thói má hồng đánh ghen\\nCảo thơm lần g'\n",
            "'iở trước đèn\\nPhong tình có lục còn truyền sử xanh\\nRằng năm Gia Tĩnh triều Minh\\nBốn phương phẳng lặng'\n",
            "' hai kinh vững vàng\\nCó nhà viên ngoại họ Vương\\nGia tư nghĩ cũng thường thường bực trung\\nMột trai con'\n",
            "' thứ rốt lòng\\nVương Quan là chữ nối dòng nho gia\\nĐầu lòng hai ả tố nga\\nThúy Kiều là chị em là Thúy V'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nn0GRa141zB2",
        "colab_type": "text"
      },
      "source": [
        "# Create input sequence and target sequence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6uu3_nPb6kzB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNTKWGIW84vh",
        "colab_type": "code",
        "outputId": "03f03372-a963-4942-e6f3-2d34b31706c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "for input_example, target_example in  dataset.take(1):\n",
        "  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
        "  print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input data:  'Trăm năm trong cõi người ta\\nChữ tài chữ mệnh khéo là ghét nhau\\nTrải qua một cuộc bể dâu\\nNhững điều '\n",
            "Target data: 'răm năm trong cõi người ta\\nChữ tài chữ mệnh khéo là ghét nhau\\nTrải qua một cuộc bể dâu\\nNhững điều t'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mX81xodt2Qt1",
        "colab_type": "text"
      },
      "source": [
        "# Create a tf.data dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92zPpUdY88kP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LQgNRLT9aWr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KaenxSxV2cid",
        "colab_type": "text"
      },
      "source": [
        "# Create simple model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKrgCH3k9c0Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.GRU(rnn_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7C7pCoVB9eiJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(\n",
        "  vocab_size = len(vocab),\n",
        "  embedding_dim=embedding_dim,\n",
        "  rnn_units=rnn_units,\n",
        "  batch_size=BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGGvp1W-9ias",
        "colab_type": "code",
        "outputId": "5046b4e1-44c2-4b2a-ce86-8e0e8e61efcf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_11 (Embedding)     (64, None, 256)           32000     \n",
            "_________________________________________________________________\n",
            "gru_11 (GRU)                 (64, None, 1024)          3938304   \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (64, None, 125)           128125    \n",
            "=================================================================\n",
            "Total params: 4,098,429\n",
            "Trainable params: 4,098,429\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntsP4TlJ9mB6",
        "colab_type": "code",
        "outputId": "aecb56b4-6df9-4d6f-8b44-411621bd6e08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction shape:  (64, 100, 125)  # (batch_size, sequence_length, vocab_size)\n",
            "scalar_loss:       4.8289127\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Z101duB9pxM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvxEPk__2lAS",
        "colab_type": "text"
      },
      "source": [
        "# Checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIitjbM79rnM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQpyjAgu9tHX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS=150"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArqMkTIG9uqM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tp_RrvOL9viA",
        "colab_type": "code",
        "outputId": "33fa11b7-9c66-4cb0-a354-880d17ca44f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir)\n",
        "model1 = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "model1.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "model1.build(tf.TensorShape([1, None]))\n",
        "model1.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_14\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_14 (Embedding)     (1, None, 256)            32000     \n",
            "_________________________________________________________________\n",
            "gru_14 (GRU)                 (1, None, 1024)           3938304   \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (1, None, 125)            128125    \n",
            "=================================================================\n",
            "Total params: 4,098,429\n",
            "Trainable params: 4,098,429\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fB9_zMYR2pja",
        "colab_type": "text"
      },
      "source": [
        "# Eveluate Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9oLyenXKZSMQ",
        "colab_type": "code",
        "outputId": "08c1751f-7bba-4c63-8d4d-bf4a2d34a4a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate = 1000\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        "  # Experiment to find the best setting.\n",
        "  temperature = 1.0\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        " \n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "\n",
        "      # remove the batch dimension\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "      \n",
        "      # using a categorical distribution to predict the word returned by the model\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "      \n",
        "      # We pass the predicted word as the next input to the model\n",
        "      # along with the previous hidden state\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))\n",
        "\n",
        "print(generate_text(model1, start_string=u\"Trăm\"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trămới biết nghĩ sau\n",
            "Người lên ngựa kẻ chia bào\n",
            "Rừng phong thuận dùng dãy đòng bỏ chững nhiều\n",
            "Đặm trường thần sách mồn mang\n",
            "Trong nào nỡ dứt nghĩa người ra đi\n",
            "ông rằng: Bỉ thử nhất thì\n",
            "Tu hàng mở cỏ nhìn lòng\n",
            "Tạ ân lạy trước Từ công:\n",
            "Chút thân bồ liễu nành hoa chưa về\n",
            "Cửa ngoài vội rủ rèm the\n",
            "Xăm xăm băng lối nước cao thâm dường\n",
            "Lại tìm những chốn đoạn trường mà đi\n",
            "Hết nạn ấy đến nạn kiếp phương trời đi tình biết một mình mình hay\n",
            "Làm cho sốn đọc thoang thịức đường tơ lụ mười viên tai\n",
            "Điều đâu nói lạ dường này\n",
            "Sự nàng đã biết tình chi đây\\\n",
            "Rằng: Lòng đương thổn thức đầy\n",
            "Tơ duyên còn vướng tơi bời\n",
            "Mặt duyên đâu Hãy chơ người thước cao\n",
            "Được lời mụ mới tùy cơ nguyền thầm xuân đã quên thân\n",
            "Một nhà để chị riêng oan một mình\n",
            "Chẳng ngờ gã Mã Giám Sinh\n",
            "Với chi lá chống những người \n",
            "Thư khói việc nàng về xem ý tứ nhà\n",
            "Sự mình cũng rắp lân la giải giở nhìn đâu chân gia\n",
            "Thanh lời nàng mới lựa dây\n",
            "Nỉ nha đã dạt xa xăn bót tỏi trời tri \n",
            "đảo nhiều lại gặp thế nào\n",
            "Bại ra thế ấy vịnh vào thế kia \n",
            "Phong trầ\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}